# Copyright (c) 2021-2024, The RSL-RL Project Developers.
# All rights reserved.
# Original code is licensed under the BSD-3-Clause license.
#
# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# Copyright (c) 2025-2026, The Legged Lab Project Developers.
# All rights reserved.
#
# Copyright (c) 2025-2026, The TienKung-Lab Project Developers.
# All rights reserved.
# Modifications are licensed under the BSD-3-Clause license.
#
# This file contains code derived from the RSL-RL, Isaac Lab, and Legged Lab Projects,
# with additional modifications by the TienKung-Lab Project,
# and is distributed under the BSD-3-Clause license.

import argparse
import os
import sys

import mujoco
import mujoco_viewer
import numpy as np
import torch
from pynput import keyboard
import time

def pd_control(target_q, q, kp, target_dq, dq, kd):
    """Calculates torques from position commands"""
    return (target_q - q) * kp + (target_dq - dq) * kd

class SimToSimCfg:
    """Configuration class for sim2sim parameters.

    Must be kept consistent with the training configuration.
    """

    class sim:
        sim_duration = 100.0
        num_action = 23
        num_obs_per_step = 84
        actor_obs_history_length = 10
        dt = 0.005
        decimation = 4
        clip_observations = 100.0
        clip_actions = 100.0
        action_scale = 0.25

    class robot:
        gait_air_ratio_l: float = 0.38
        gait_air_ratio_r: float = 0.38
        gait_phase_offset_l: float = 0.38
        gait_phase_offset_r: float = 0.88
        gait_cycle: float = 0.85


class MujocoRunner:
    """
    Sim2Sim runner that loads a policy and a MuJoCo model
    to run real-time humanoid control simulation.

    Args:
        cfg (SimToSimCfg): Configuration object for simulation.
        policy_path (str): Path to the TorchScript exported policy.
        model_path (str): Path to the MuJoCo XML model.
    """

    def __init__(self, cfg: SimToSimCfg, policy_path, model_path):
        self.cfg = cfg
        network_path = policy_path
        self.model = mujoco.MjModel.from_xml_path(model_path)
        self.model.opt.timestep = self.cfg.sim.dt

        self.policy = torch.jit.load(network_path)
        self.data = mujoco.MjData(self.model)
        self.viewer = mujoco_viewer.MujocoViewer(self.model, self.data)
        self.viewer._render_every_frame = False
        self.init_variables()

    def init_variables(self) -> None:
        """Initialize simulation variables and joint index mappings."""
        self.dt = self.cfg.sim.decimation * self.cfg.sim.dt
        self.dof_pos = np.zeros(self.cfg.sim.num_action)
        self.dof_vel = np.zeros(self.cfg.sim.num_action)
        self.action = np.zeros(self.cfg.sim.num_action)
        self.default_dof_pos = np.array(
            [
                 -0.1000,  0.0000,  0.0000,  0.3000, -0.2000,  0.0000,
                 -0.1000,  0.0000,  0.0000,  0.3000, -0.2000,  0.0000,
                  0.0000,
                  0.3000,  0.2500,  0.0000,  0.9700,  0.1500,
                  0.3000, -0.2500,  0.0000,  0.9700, -0.1500
            ]
        )
        self.episode_length_buf = 0
        self.gait_phase = np.zeros(2)
        self.gait_cycle = self.cfg.robot.gait_cycle
        self.phase_ratio = np.array([self.cfg.robot.gait_air_ratio_l, self.cfg.robot.gait_air_ratio_r])
        self.phase_offset = np.array([self.cfg.robot.gait_phase_offset_l, self.cfg.robot.gait_phase_offset_r])

        self.sim_kps = np.array([
            100., 100.,
            200.,
            100., 100.,
            40.,  40.,
            100., 100.,
            40.,  40.,
            150., 150.,
            40.,  40.,
            40.,  40.,
            40.,  40.,
            40.,  40.,
            40.,  40.
            ], dtype = np.float32)


        self.sim_kds = np.array([
            2., 2.,
            5.,
            2., 2.,
            1., 1.,
            2., 2.,
            1., 1.,
            4., 4.,
            1., 1.,
            2., 2.,
            1., 1.,
            2., 2.,
            1., 1.
            ], dtype = np.float32)

        self.sim_pos_limits = np.array([[-2.2602,  2.6093],
                                    [-2.2602,  2.6093],
                                    [-2.3562,  2.3562],
                                    [-0.3491,  2.7926],
                                    [-2.7926,  0.3491],
                                    [-2.8012,  2.3824],
                                    [-2.8012,  2.3824],
                                    [-2.4818,  2.4818],
                                    [-2.4818,  2.4818],
                                    [-1.3962,  2.0595],
                                    [-2.0595,  1.3962],
                                    [ 0.0611,  2.7314],
                                    [ 0.0611,  2.7314],
                                    [-2.3562,  2.3562],
                                    [-2.3562,  2.3562],
                                    [-0.8029,  0.4538],
                                    [-0.8029,  0.4538],
                                    [-0.8901,  1.9373],
                                    [-0.8901,  1.9373],
                                    [-0.2356,  0.2356],
                                    [-0.2356,  0.2356],
                                    [-1.7750,  1.7750],
                                    [-1.7750,  1.7750]], dtype = np.float32)

        self.sim_effort_limits = np.array(
            [ 88.,  88.,
            88.,
            139., 139.,
            25.,  25.,
            88.,  88.,
            25.,  25.,
            139., 139.,
            25.,  25.,
            35.,  35.,
            25.,  25.,
            35.,  35.,
            25.,  25.], dtype = np.float32)

        mujoco_joint_names = []
        for joint_id in range(self.model.njnt):
            joint_name = mujoco.mj_id2name(self.model, mujoco.mjtObj.mjOBJ_JOINT, joint_id)
            mujoco_joint_names.append(joint_name)

        mujoco_joint_names = mujoco_joint_names[1:]
        sim_joint_names = [
            "left_hip_pitch_joint", "right_hip_pitch_joint",
            "waist_yaw_joint",
            "left_hip_roll_joint", "right_hip_roll_joint",  #hip_roll
            "left_shoulder_pitch_joint",  "right_shoulder_pitch_joint",   #shoulder_pitch
            "left_hip_yaw_joint", "right_hip_yaw_joint",  #hip_yaw
            "left_shoulder_roll_joint",  "right_shoulder_roll_joint",   #shoulder_roll
            "left_knee_joint", "right_knee_joint",  #knee
            "left_shoulder_yaw_joint", "right_shoulder_yaw_joint",    #shoulder_yaw
            "left_ankle_pitch_joint",  "right_ankle_pitch_joint",   #ankle_pitch
            "left_elbow_joint",  "right_elbow_joint",   #elbow
            "left_ankle_roll_joint",  "right_ankle_roll_joint",   #ankle_roll
            "left_wrist_roll_joint",  "right_wrist_roll_joint"   #wrist_roll
        ]

        self.mujoco_to_isaac_idx = [mujoco_joint_names.index(name)  for name in sim_joint_names]
        self.isaac_to_mujoco_idx = [sim_joint_names.index(name)  for name in mujoco_joint_names]

        self.kps = self.sim_kps[self.isaac_to_mujoco_idx]
        self.kds = self.sim_kds[self.isaac_to_mujoco_idx]
        self.pos_limits = self.sim_pos_limits[self.isaac_to_mujoco_idx]
        self.effort_limits = self.sim_effort_limits[self.isaac_to_mujoco_idx]

        # Initial command vel
        self.command_vel = np.array([0.0, 0.0, 0.0])
        self.obs_history = np.zeros(
            (self.cfg.sim.num_obs_per_step * self.cfg.sim.actor_obs_history_length,), dtype=np.float32
        )

    def get_obs(self) -> np.ndarray:
        """
        Compute current observation vector from MuJoCo sensors and internal state.

        Returns:
            np.ndarray: Normalized and clipped observation history.
        """
        #self.dof_pos = self.data.sensordata[0:20]
        #self.dof_vel = self.data.sensordata[20:40]
        self.dof_pos[:] = self.data.qpos[7:]
        self.dof_vel[:] = self.data.qvel[6:]
        quat = self.data.qpos[3:7]
        omega = self.data.qvel[3:6]

        obs = np.concatenate(
            [
                omega.astype(np.double), #self.data.sensor("angular-velocity").data.astype(np.double),  # 3
                self.quat_rotate_inverse(
                    quat[[1, 2, 3, 0]].astype(np.double), np.array([0, 0, -1])
                ),  # 3
                self.command_vel,  # 3
                (self.dof_pos - self.default_dof_pos)[self.mujoco_to_isaac_idx],  # 20
                self.dof_vel[self.mujoco_to_isaac_idx],  # 20
                np.clip(self.action, -self.cfg.sim.clip_actions, self.cfg.sim.clip_actions),  # 20
                np.sin(2 * np.pi * self.gait_phase),  # 2
                np.cos(2 * np.pi * self.gait_phase),  # 2
                self.phase_ratio,  # 2
            ],
            axis=0,
        ).astype(np.float32)

        # Update observation history
        self.obs_history = np.roll(self.obs_history, shift=-self.cfg.sim.num_obs_per_step)
        self.obs_history[-self.cfg.sim.num_obs_per_step :] = obs.copy()

        return np.clip(self.obs_history, -self.cfg.sim.clip_observations, self.cfg.sim.clip_observations)

    def position_control(self) -> np.ndarray:
        """
        Apply position control using scaled action.

        Returns:
            np.ndarray: Target joint positions in MuJoCo order.
        """
        actions_scaled = self.action * self.cfg.sim.action_scale
        target_pos = actions_scaled[self.isaac_to_mujoco_idx] + self.default_dof_pos
        clip_pos = np.clip(target_pos, self.pos_limits[:, 0], self.pos_limits[:, 1])

        diff_pos = target_pos - clip_pos
        print("diff pos", diff_pos)
        return clip_pos

    def run(self) -> None:
        """
        Run the simulation loop with keyboard-controlled commands.
        """
        self.setup_keyboard_listener()
        self.listener.start()

        while self.data.time < self.cfg.sim.sim_duration:
            self.obs_history = self.get_obs()
            self.action[:] = self.policy(torch.tensor(self.obs_history, dtype=torch.float32)).detach().numpy()
            self.action = np.clip(self.action, -self.cfg.sim.clip_actions, self.cfg.sim.clip_actions)

            target_dof_pos = self.position_control()

            for sim_update in range(self.cfg.sim.decimation):
                step_start_time = time.time()

                tau = pd_control(target_dof_pos, self.data.qpos[7:], self.kps, np.zeros_like(self.kds), self.data.qvel[6:], self.kds)
                clip_tau = np.clip(tau, - self.effort_limits, self.effort_limits)
                print("diff effort", clip_tau)

                self.data.ctrl = clip_tau
                mujoco.mj_step(self.model, self.data)
                self.viewer.render()

                elapsed = time.time() - step_start_time
                sleep_time = self.cfg.sim.dt - elapsed
                if sleep_time > 0:
                    time.sleep(sleep_time)
            self.episode_length_buf += 1
            self.calculate_gait_para()

        self.listener.stop()
        self.viewer.close()

    def quat_rotate_inverse(self, q: np.ndarray, v: np.ndarray) -> np.ndarray:
        """
        Rotate a vector by the inverse of a quaternion.

        Args:
            q (np.ndarray): Quaternion (x, y, z, w) format.
            v (np.ndarray): Vector to rotate.

        Returns:
            np.ndarray: Rotated vector.
        """
        q_w = q[-1]
        q_vec = q[:3]
        a = v * (2.0 * q_w**2 - 1.0)
        b = np.cross(q_vec, v) * q_w * 2.0
        c = q_vec * np.dot(q_vec, v) * 2.0

        return a - b + c

    def calculate_gait_para(self) -> None:
        """
        Update gait phase parameters based on simulation time and offset.
        """
        t = self.episode_length_buf * self.dt / self.gait_cycle
        self.gait_phase[0] = (t + self.phase_offset[0]) % 1.0
        self.gait_phase[1] = (t + self.phase_offset[1]) % 1.0

    def adjust_command_vel(self, idx: int, increment: float) -> None:
        """
        Adjust command velocity vector.

        Args:
            idx (int): Index of velocity component (0=x, 1=y, 2=yaw).
            increment (float): Value to increment.
        """
        self.command_vel[idx] += increment
        self.command_vel[idx] = np.clip(self.command_vel[idx], -1.0, 1.0)  # vel clip

    def setup_keyboard_listener(self) -> None:
        """
        Set up keyboard event listener for user control input.
        """

        def on_press(key):
            try:
                if key.char == "8":  # NumPad 8      x += 0.2
                    self.adjust_command_vel(0, 0.2)
                elif key.char == "2":  # NumPad 2      x -= 0.2
                    self.adjust_command_vel(0, -0.2)
                elif key.char == "4":  # NumPad 4      y -= 0.2
                    self.adjust_command_vel(1, -0.2)
                elif key.char == "6":  # NumPad 6      y += 0.2
                    self.adjust_command_vel(1, 0.2)
                elif key.char == "7":  # NumPad 7      yaw += 0.2
                    self.adjust_command_vel(2, -0.2)
                elif key.char == "9":  # NumPad 9      yaw -= 0.2
                    self.adjust_command_vel(2, 0.2)
            except AttributeError:
                pass

        self.listener = keyboard.Listener(on_press=on_press)


if __name__ == "__main__":
    LEGGED_LAB_ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
    parser = argparse.ArgumentParser(description="Run sim2sim Mujoco controller.")
    parser.add_argument(
        "--task",
        type=str,
        default="walk",
        choices=["walk", "run"],
        help="Task type: 'walk' or 'run' to set gait parameters",
    )
    parser.add_argument(
        "--policy",
        type=str,
        default=None,
        help="Path to policy.pt. If not specified, it will be set automatically based on --task",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=os.path.join(LEGGED_LAB_ROOT_DIR, "legged_lab/assets/tienkung2_lite/mjcf/tienkung.xml"),
        help="Path to model.xml",
    )
    parser.add_argument("--duration", type=float, default=100.0, help="Simulation duration in seconds")
    args = parser.parse_args()

    args.task = "walk_g123dof"
    args.policy = "logs/g1_23dof_walk/2025-12-21_20-05-49/exported/policy.pt"
    args.model = "legged_lab/assets/unitree/urdf/g1_description/g1_23dof_rev_1_0.xml"
    args.duration = 100


    if args.policy is None:
        args.policy = os.path.join(LEGGED_LAB_ROOT_DIR, "Exported_policy", f"{args.task}.pt")

    if not os.path.isfile(args.policy):
        print(f"[ERROR] Policy file not found: {args.policy}")
        sys.exit(1)
    if not os.path.isfile(args.model):
        print(f"[ERROR] MuJoCo model file not found: {args.model}")
        sys.exit(1)

    print(f"[INFO] Loaded task preset: {args.task.upper()}")
    print(f"[INFO] Loaded policy: {args.policy}")
    print(f"[INFO] Loaded model: {args.model}")

    sim_cfg = SimToSimCfg()
    sim_cfg.sim.sim_duration = args.duration

    # Set gait parameters according to task
    if args.task == "walk_g123dof":
        sim_cfg.robot.gait_air_ratio_l = 0.38
        sim_cfg.robot.gait_air_ratio_r = 0.38
        sim_cfg.robot.gait_phase_offset_l = 0.38
        sim_cfg.robot.gait_phase_offset_r = 0.88
        sim_cfg.robot.gait_cycle = 0.85
    elif args.task == "run":
        sim_cfg.robot.gait_air_ratio_l = 0.6
        sim_cfg.robot.gait_air_ratio_r = 0.6
        sim_cfg.robot.gait_phase_offset_l = 0.6
        sim_cfg.robot.gait_phase_offset_r = 0.1
        sim_cfg.robot.gait_cycle = 0.5

    runner = MujocoRunner(
        cfg=sim_cfg,
        policy_path=args.policy,
        model_path=args.model,
    )
    runner.run()
