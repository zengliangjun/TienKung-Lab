# Copyright (c) 2021-2024, The RSL-RL Project Developers.
# All rights reserved.
# Original code is licensed under the BSD-3-Clause license.
#
# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# Copyright (c) 2025-2026, The Legged Lab Project Developers.
# All rights reserved.
#
# Copyright (c) 2025-2026, The TienKung-Lab Project Developers.
# All rights reserved.
# Modifications are licensed under the BSD-3-Clause license.
#
# This file contains code derived from the RSL-RL, Isaac Lab, and Legged Lab Projects,
# with additional modifications by the TienKung-Lab Project,
# and is distributed under the BSD-3-Clause license.

import argparse
import os
import sys

import mujoco
import mujoco_viewer
import numpy as np
import torch
from pynput import keyboard
import time

def pd_control(target_q, q, kp, target_dq, dq, kd):
    """Calculates torques from position commands"""
    return (target_q - q) * kp + (target_dq - dq) * kd

class SimToSimCfg:
    """Configuration class for sim2sim parameters.

    Must be kept consistent with the training configuration.
    """

    class sim:
        sim_duration = 100.0
        num_action = 20
        num_obs_per_step = 75
        actor_obs_history_length = 10
        dt = 0.005
        decimation = 4
        clip_observations = 100.0
        clip_actions = 100.0
        action_scale = 0.25

    class robot:
        gait_air_ratio_l: float = 0.38
        gait_air_ratio_r: float = 0.38
        gait_phase_offset_l: float = 0.38
        gait_phase_offset_r: float = 0.88
        gait_cycle: float = 0.85


class MujocoRunner:
    """
    Sim2Sim runner that loads a policy and a MuJoCo model
    to run real-time humanoid control simulation.

    Args:
        cfg (SimToSimCfg): Configuration object for simulation.
        policy_path (str): Path to the TorchScript exported policy.
        model_path (str): Path to the MuJoCo XML model.
    """

    def __init__(self, cfg: SimToSimCfg, policy_path, model_path):
        self.cfg = cfg
        network_path = policy_path
        self.model = mujoco.MjModel.from_xml_path(model_path)
        self.model.opt.timestep = self.cfg.sim.dt

        self.policy = torch.jit.load(network_path)
        self.data = mujoco.MjData(self.model)
        self.viewer = mujoco_viewer.MujocoViewer(self.model, self.data)
        self.viewer._render_every_frame = False
        self.init_variables()

    def init_variables(self) -> None:
        """Initialize simulation variables and joint index mappings."""
        self.dt = self.cfg.sim.decimation * self.cfg.sim.dt
        self.dof_pos = np.zeros(self.cfg.sim.num_action)
        self.dof_vel = np.zeros(self.cfg.sim.num_action)
        self.action = np.zeros(self.cfg.sim.num_action)
        self.default_dof_pos = np.array(
            [0, -0.5, 0, 1.0, -0.5, 0, 0, -0.5, 0, 1.0, -0.5, 0, 0, 0.1, 0.0, -0.3, 0, -0.1, 0.0, -0.3]
        )
        self.episode_length_buf = 0
        self.gait_phase = np.zeros(2)
        self.gait_cycle = self.cfg.robot.gait_cycle
        self.phase_ratio = np.array([self.cfg.robot.gait_air_ratio_l, self.cfg.robot.gait_air_ratio_r])
        self.phase_offset = np.array([self.cfg.robot.gait_phase_offset_l, self.cfg.robot.gait_phase_offset_r])

        self.sim_kps = np.array([
            700.0000, 700.0000,  60.0000,  60.0000, 700.0000, 700.0000,  20.0000,
            20.0000, 500.0000, 500.0000,  10.0000,  10.0000, 700.0000, 700.0000,
            10.0000,  10.0000,  30.0000,  30.0000,  16.8000,  16.8000
            ], dtype = np.float32)


        self.sim_kds = np.array([
                10.0000, 10.0000,  3.0000,  3.0000, 10.0000, 10.0000,  1.5000,  1.5000,
                5.0000,  5.0000,  1.0000,  1.0000, 10.0000, 10.0000,  1.0000,  1.0000,
                2.5000,  2.5000,  1.4000,  1.4000
            ], dtype = np.float32)

        self.sim_pos_limits = np.array([[-0.7110,  0.7110],
                                        [-0.7110,  0.7110],
                                        [-2.6640,  2.6640],
                                        [-2.6640,  2.6640],
                                        [-2.5460,  1.8460],
                                        [-2.5460,  1.8460],
                                        [-0.0787,  3.2169],
                                        [-3.2169,  0.0787],
                                        [-0.9360,  0.9360],
                                        [-0.9360,  0.9360],
                                        [-2.6640,  2.6640],
                                        [-2.6640,  2.6640],
                                        [ 0.1195,  2.2705],
                                        [ 0.1195,  2.2705],
                                        [-2.4664,  0.1174],
                                        [-2.4664,  0.1174],
                                        [-1.1328,  0.4364],
                                        [-1.1328,  0.4364],
                                        [-0.3927,  0.3927],
                                        [-0.3927,  0.3927]], dtype = np.float32)

        self.sim_effort_limits = np.array(
            [180.0000, 180.0000,  52.5000,  52.5000, 300.0000, 300.0000,  52.5000,
             52.5000, 180.0000, 180.0000,  52.5000,  52.5000, 300.0000, 300.0000,
             52.5000,  52.5000,  60.0000,  60.0000,  30.0000,  30.0000], dtype = np.float32)


        self.mujoco_to_isaac_idx = [
            0,  # hip_roll_l_joint
            6,  # hip_roll_r_joint
            12,  # shoulder_pitch_l_joint
            16,  # shoulder_pitch_r_joint
            1,  # hip_pitch_l_joint
            7,  # hip_pitch_r_joint
            13,  # shoulder_roll_l_joint
            17,  # shoulder_roll_r_joint
            2,  # hip_yaw_l_joint
            8,  # hip_yaw_r_joint
            14,  # shoulder_yaw_l_joint
            18,  # shoulder_yaw_r_joint
            3,  # knee_pitch_l_joint
            9,  # knee_pitch_r_joint
            15,  # elbow_pitch_l_joint
            19,  # elbow_pitch_r_joint
            4,  # ankle_pitch_l_joint
            10,  # ankle_pitch_r_joint
            5,  # ankle_roll_l_joint
            11,  # ankle_roll_r_joint
        ]
        self.isaac_to_mujoco_idx = [
            0,  # hip_roll_l_joint
            4,  # hip_pitch_l_joint
            8,  # hip_yaw_l_joint
            12,  # knee_pitch_l_joint
            16,  # ankle_pitch_l_joint
            18,  # ankle_roll_l_joint
            1,  # hip_roll_r_joint
            5,  # hip_pitch_r_joint
            9,  # hip_yaw_r_joint
            13,  # knee_pitch_r_joint
            17,  # ankle_pitch_r_joint
            19,  # ankle_roll_r_joint
            2,  # shoulder_pitch_l_joint
            6,  # shoulder_roll_l_joint
            10,  # shoulder_yaw_l_joint
            14,  # elbow_pitch_l_joint
            3,  # shoulder_pitch_r_joint
            7,  # shoulder_roll_r_joint
            11,  # shoulder_yaw_r_joint
            15,  # elbow_pitch_r_joint
        ]

        self.kps = self.sim_kps[self.isaac_to_mujoco_idx]
        self.kds = self.sim_kds[self.isaac_to_mujoco_idx]
        self.pos_limits = self.sim_pos_limits[self.isaac_to_mujoco_idx]
        self.effort_limits = self.sim_effort_limits[self.isaac_to_mujoco_idx]

        # Initial command vel
        self.command_vel = np.array([0.0, 0.0, 0.0])
        self.obs_history = np.zeros(
            (self.cfg.sim.num_obs_per_step * self.cfg.sim.actor_obs_history_length,), dtype=np.float32
        )

    def get_obs(self) -> np.ndarray:
        """
        Compute current observation vector from MuJoCo sensors and internal state.

        Returns:
            np.ndarray: Normalized and clipped observation history.
        """
        #self.dof_pos = self.data.sensordata[0:20]
        #self.dof_vel = self.data.sensordata[20:40]
        self.dof_pos[:] = self.data.qpos[7:]
        self.dof_vel[:] = self.data.qvel[6:]
        quat = self.data.qpos[3:7]
        omega = self.data.qvel[3:6]

        obs = np.concatenate(
            [
                omega.astype(np.double), #self.data.sensor("angular-velocity").data.astype(np.double),  # 3
                self.quat_rotate_inverse(
                    quat[[1, 2, 3, 0]].astype(np.double), np.array([0, 0, -1])
                ),  # 3
                self.command_vel,  # 3
                (self.dof_pos - self.default_dof_pos)[self.mujoco_to_isaac_idx],  # 20
                self.dof_vel[self.mujoco_to_isaac_idx],  # 20
                np.clip(self.action, -self.cfg.sim.clip_actions, self.cfg.sim.clip_actions),  # 20
                np.sin(2 * np.pi * self.gait_phase),  # 2
                np.cos(2 * np.pi * self.gait_phase),  # 2
                self.phase_ratio,  # 2
            ],
            axis=0,
        ).astype(np.float32)

        # Update observation history
        self.obs_history = np.roll(self.obs_history, shift=-self.cfg.sim.num_obs_per_step)
        self.obs_history[-self.cfg.sim.num_obs_per_step :] = obs.copy()

        return np.clip(self.obs_history, -self.cfg.sim.clip_observations, self.cfg.sim.clip_observations)

    def position_control(self) -> np.ndarray:
        """
        Apply position control using scaled action.

        Returns:
            np.ndarray: Target joint positions in MuJoCo order.
        """
        actions_scaled = self.action * self.cfg.sim.action_scale
        target_pos = actions_scaled[self.isaac_to_mujoco_idx] + self.default_dof_pos
        clip_pos = np.clip(target_pos, self.pos_limits[:, 0], self.pos_limits[:, 1])

        diff_pos = target_pos - clip_pos
        print("diff pos", diff_pos)
        return clip_pos

    def run(self) -> None:
        """
        Run the simulation loop with keyboard-controlled commands.
        """
        self.setup_keyboard_listener()
        self.listener.start()

        while self.data.time < self.cfg.sim.sim_duration:
            self.obs_history = self.get_obs()
            self.action[:] = self.policy(torch.tensor(self.obs_history, dtype=torch.float32)).detach().numpy()[:20]
            self.action = np.clip(self.action, -self.cfg.sim.clip_actions, self.cfg.sim.clip_actions)

            target_dof_pos = self.position_control()

            for sim_update in range(self.cfg.sim.decimation):
                step_start_time = time.time()

                tau = pd_control(target_dof_pos, self.data.qpos[7:], self.kps, np.zeros_like(self.kds), self.data.qvel[6:], self.kds)
                clip_tau = np.clip(tau, - self.effort_limits, self.effort_limits)

                self.data.ctrl = clip_tau
                mujoco.mj_step(self.model, self.data)
                self.viewer.render()

                elapsed = time.time() - step_start_time
                sleep_time = self.cfg.sim.dt - elapsed
                if sleep_time > 0:
                    time.sleep(sleep_time)
            self.episode_length_buf += 1
            self.calculate_gait_para()

        self.listener.stop()
        self.viewer.close()

    def quat_rotate_inverse(self, q: np.ndarray, v: np.ndarray) -> np.ndarray:
        """
        Rotate a vector by the inverse of a quaternion.

        Args:
            q (np.ndarray): Quaternion (x, y, z, w) format.
            v (np.ndarray): Vector to rotate.

        Returns:
            np.ndarray: Rotated vector.
        """
        q_w = q[-1]
        q_vec = q[:3]
        a = v * (2.0 * q_w**2 - 1.0)
        b = np.cross(q_vec, v) * q_w * 2.0
        c = q_vec * np.dot(q_vec, v) * 2.0

        return a - b + c

    def calculate_gait_para(self) -> None:
        """
        Update gait phase parameters based on simulation time and offset.
        """
        t = self.episode_length_buf * self.dt / self.gait_cycle
        self.gait_phase[0] = (t + self.phase_offset[0]) % 1.0
        self.gait_phase[1] = (t + self.phase_offset[1]) % 1.0

    def adjust_command_vel(self, idx: int, increment: float) -> None:
        """
        Adjust command velocity vector.

        Args:
            idx (int): Index of velocity component (0=x, 1=y, 2=yaw).
            increment (float): Value to increment.
        """
        self.command_vel[idx] += increment
        self.command_vel[idx] = np.clip(self.command_vel[idx], -1.0, 1.0)  # vel clip

    def setup_keyboard_listener(self) -> None:
        """
        Set up keyboard event listener for user control input.
        """

        def on_press(key):
            try:
                if key.char == "8":  # NumPad 8      x += 0.2
                    self.adjust_command_vel(0, 0.2)
                elif key.char == "2":  # NumPad 2      x -= 0.2
                    self.adjust_command_vel(0, -0.2)
                elif key.char == "4":  # NumPad 4      y -= 0.2
                    self.adjust_command_vel(1, -0.2)
                elif key.char == "6":  # NumPad 6      y += 0.2
                    self.adjust_command_vel(1, 0.2)
                elif key.char == "7":  # NumPad 7      yaw += 0.2
                    self.adjust_command_vel(2, -0.2)
                elif key.char == "9":  # NumPad 9      yaw -= 0.2
                    self.adjust_command_vel(2, 0.2)
            except AttributeError:
                pass

        self.listener = keyboard.Listener(on_press=on_press)


if __name__ == "__main__":
    LEGGED_LAB_ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
    parser = argparse.ArgumentParser(description="Run sim2sim Mujoco controller.")
    parser.add_argument(
        "--task",
        type=str,
        default="walk",
        choices=["walk", "run"],
        help="Task type: 'walk' or 'run' to set gait parameters",
    )
    parser.add_argument(
        "--policy",
        type=str,
        default=None,
        help="Path to policy.pt. If not specified, it will be set automatically based on --task",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=os.path.join(LEGGED_LAB_ROOT_DIR, "legged_lab/assets/tienkung2_lite/mjcf/tienkung.xml"),
        help="Path to model.xml",
    )
    parser.add_argument("--duration", type=float, default=100.0, help="Simulation duration in seconds")
    args = parser.parse_args()

    args.task = "walk"
    args.policy = "Exported_policy/walk.pt"
    args.model = "legged_lab/assets/tienkung2_lite/mjcf/tienkung_v2.xml"
    args.duration = 100


    if args.policy is None:
        args.policy = os.path.join(LEGGED_LAB_ROOT_DIR, "Exported_policy", f"{args.task}.pt")

    if not os.path.isfile(args.policy):
        print(f"[ERROR] Policy file not found: {args.policy}")
        sys.exit(1)
    if not os.path.isfile(args.model):
        print(f"[ERROR] MuJoCo model file not found: {args.model}")
        sys.exit(1)

    print(f"[INFO] Loaded task preset: {args.task.upper()}")
    print(f"[INFO] Loaded policy: {args.policy}")
    print(f"[INFO] Loaded model: {args.model}")

    sim_cfg = SimToSimCfg()
    sim_cfg.sim.sim_duration = args.duration

    # Set gait parameters according to task
    if args.task == "walk":
        sim_cfg.robot.gait_air_ratio_l = 0.38
        sim_cfg.robot.gait_air_ratio_r = 0.38
        sim_cfg.robot.gait_phase_offset_l = 0.38
        sim_cfg.robot.gait_phase_offset_r = 0.88
        sim_cfg.robot.gait_cycle = 0.85
    elif args.task == "run":
        sim_cfg.robot.gait_air_ratio_l = 0.6
        sim_cfg.robot.gait_air_ratio_r = 0.6
        sim_cfg.robot.gait_phase_offset_l = 0.6
        sim_cfg.robot.gait_phase_offset_r = 0.1
        sim_cfg.robot.gait_cycle = 0.5

    runner = MujocoRunner(
        cfg=sim_cfg,
        policy_path=args.policy,
        model_path=args.model,
    )
    runner.run()
